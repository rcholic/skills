"""
Detail Fetcher - æŠ“å–æŠ€èƒ½è¯¦æƒ…é¡µ
"""
import re
import time
from typing import Dict, List, Optional
from bs4 import BeautifulSoup
import requests

from src.config import FETCH_REQUEST_DELAY, SKILLS_BASE_URL


class DetailFetcher:
    """æŠ“å–æŠ€èƒ½è¯¦æƒ…é¡µ"""

    def __init__(self, timeout: int = 30, delay: float = None):
        """
        åˆå§‹åŒ–

        Args:
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            delay: è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„å€¼
        """
        self.base_url = SKILLS_BASE_URL
        self.timeout = timeout
        self.delay = delay if delay is not None else FETCH_REQUEST_DELAY
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (compatible; SkillsTrendingBot/1.0)"
        })

    def fetch_top20_details(self, skills: List[Dict]) -> List[Dict]:
        """
        æ‰¹é‡æŠ“å– Top 20 è¯¦æƒ…

        Args:
            skills: Top 20 æŠ€èƒ½åˆ—è¡¨

        Returns:
            [
                {
                    "name": "remotion-best-practices",
                    "owner": "remotion-dev/skills",
                    "url": "...",
                    "html_content": "<html>...</html>",
                    "when_to_use": "Use this skills whenever...",
                    "rules": [
                        {"file": "3d.md", "desc": "3D content in Remotion..."},
                        ...
                    ],
                    "rules_count": 27
                },
                ...
            ]
        """
        results = []
        top_n = min(20, len(skills))

        print(f"ğŸ“¥ å¼€å§‹æŠ“å– Top {top_n} è¯¦æƒ…...")

        for i, skill in enumerate(skills[:top_n], 1):
            url = skill.get("url", "")
            if not url:
                # å°è¯•æ„å»º URL
                name = skill.get("name", "")
                owner = skill.get("owner", "")
                url = f"{self.base_url}/{owner}/{name}"

            print(f"  [{i}/{top_n}] æŠ“å–: {skill.get('name')}")

            detail = self.fetch_detail_page(url, skill)
            if detail:
                results.append(detail)
            else:
                # å³ä½¿å¤±è´¥ä¹Ÿä¿ç•™åŸºæœ¬ä¿¡æ¯
                results.append({
                    "name": skill.get("name"),
                    "owner": skill.get("owner"),
                    "url": url,
                    "when_to_use": "",
                    "rules": [],
                    "rules_count": 0,
                    "error": "Failed to fetch details"
                })

            # é™é€Ÿ
            if i < top_n:
                time.sleep(self.delay)

        print(f"âœ… æˆåŠŸæŠ“å– {len(results)} ä¸ªæŠ€èƒ½è¯¦æƒ…")
        return results

    def fetch_detail_page(self, url: str, skill_info: Dict = None) -> Optional[Dict]:
        """
        è·å–å•ä¸ªæŠ€èƒ½è¯¦æƒ…

        Args:
            url: æŠ€èƒ½è¯¦æƒ…é¡µ URL
            skill_info: æŠ€èƒ½åŸºæœ¬ä¿¡æ¯

        Returns:
            æŠ€èƒ½è¯¦æƒ…å­—å…¸æˆ– None
        """
        if not skill_info:
            skill_info = {}

        try:
            response = self.session.get(url, timeout=self.timeout)
            response.raise_for_status()

            html_content = response.text

            # è§£æé¡µé¢
            detail = self.parse_detail_page(html_content, url, skill_info)
            return detail

        except requests.RequestException as e:
            print(f"    âš ï¸ è¯·æ±‚å¤±è´¥: {e}")
            return None
        except Exception as e:
            print(f"    âš ï¸ è§£æå¤±è´¥: {e}")
            return None

    def parse_detail_page(self, html_content: str, url: str, skill_info: Dict) -> Dict:
        """
        è§£ææŠ€èƒ½è¯¦æƒ…é¡µ

        Args:
            html_content: é¡µé¢ HTML
            url: é¡µé¢ URL
            skill_info: æŠ€èƒ½åŸºæœ¬ä¿¡æ¯

        Returns:
            æŠ€èƒ½è¯¦æƒ…å­—å…¸
        """
        soup = BeautifulSoup(html_content, "lxml")

        # æå– "When to use" éƒ¨åˆ†
        when_to_use = self._extract_when_to_use(soup)

        # æå–è§„åˆ™åˆ—è¡¨
        rules = self._extract_rules(soup)

        # æå–æŠ€èƒ½åç§°ï¼ˆå¦‚æœæœªæä¾›ï¼‰
        name = skill_info.get("name")
        if not name:
            name = self._extract_name_from_soup(soup, url)

        # æå–æ‹¥æœ‰è€…
        owner = skill_info.get("owner", "unknown")

        return {
            "name": name,
            "owner": owner,
            "url": url,
            "html_content": html_content,
            "when_to_use": when_to_use,
            "rules": rules,
            "rules_count": len(rules)
        }

    def _extract_when_to_use(self, soup: BeautifulSoup) -> str:
        """
        æå– "When to use" éƒ¨åˆ†

        Args:
            soup: BeautifulSoup å¯¹è±¡

        Returns:
            when_to_use æ–‡æœ¬
        """
        # å°è¯•ä¸åŒçš„é€‰æ‹©å™¨
        selectors = [
            "h2#when-to-use",
            '[id="when-to-use"]',
            "h2:contains('When to use')",
            "h2:contains('When to Use')",
        ]

        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                # è·å–ä¸‹ä¸€ä¸ªå…„å¼Ÿå…ƒç´ ï¼ˆé€šå¸¸æ˜¯å†…å®¹ï¼‰
                content = element.find_next_sibling()
                if content:
                    return content.get_text(strip=True)

        # å¦‚æœæ‰¾ä¸åˆ°æ ‡é¢˜ï¼Œå°è¯•åœ¨æ•´ä¸ªé¡µé¢ä¸­æœç´¢
        text = soup.get_text()
        match = re.search(r'When to use\s*\n\s*(.+?)(?:\n\s*##|\n\s*###|\Z)', text, re.DOTALL | re.IGNORECASE)
        if match:
            return match.group(1).strip()

        return ""

    def _extract_rules(self, soup: BeautifulSoup) -> List[Dict]:
        """
        æå–è§„åˆ™åˆ—è¡¨

        Args:
            soup: BeautifulSoup å¯¹è±¡

        Returns:
            è§„åˆ™åˆ—è¡¨
        """
        rules = []

        # å°è¯•æ‰¾åˆ°è§„åˆ™åˆ—è¡¨
        # é€šå¸¸åœ¨ "How to use" æˆ–ç±»ä¼¼çš„æ ‡é¢˜ä¸‹
        list_selectors = [
            "ul",
            "ol",
            '[class*="rules"]',
            '[class*="list"]',
        ]

        for selector in list_selectors:
            lists = soup.select(selector)

            for lst in lists:
                items = lst.find_all("li", recursive=False)
                if len(items) >= 3:  # è‡³å°‘ 3 é¡¹æ‰è®¤ä¸ºæ˜¯è§„åˆ™åˆ—è¡¨
                    for item in items:
                        link = item.find("a", href=True)
                        if link:
                            href = link.get("href", "")
                            text = link.get_text(strip=True)
                            # æè¿°å¯èƒ½åœ¨é“¾æ¥åé¢
                            desc = item.get_text(strip=True).replace(text, "", 1).strip()

                            rules.append({
                                "file": href.split("/")[-1] if href else text,
                                "desc": desc or text
                            })

                    if rules:
                        return rules

        # å¤‡ç”¨æ–¹æ¡ˆï¼šä» HTML ä¸­æå–æ‰€æœ‰çœ‹èµ·æ¥åƒè§„åˆ™çš„é“¾æ¥
        pattern = r'rules/([a-z0-9_-]+)\.md'
        matches = re.finditer(pattern, str(soup))

        for match in matches:
            rules.append({
                "file": f"{match.group(1)}.md",
                "desc": f"Rule: {match.group(1)}"
            })

        return rules

    def _extract_name_from_soup(self, soup: BeautifulSoup, url: str) -> str:
        """
        ä» URL æˆ–é¡µé¢ä¸­æå–æŠ€èƒ½åç§°

        Args:
            soup: BeautifulSoup å¯¹è±¡
            url: é¡µé¢ URL

        Returns:
            æŠ€èƒ½åç§°
        """
        # ä» URL æå–
        parts = url.strip("/").split("/")
        if len(parts) >= 1:
            return parts[-1]

        # ä»é¡µé¢æ ‡é¢˜æå–
        title = soup.find("title")
        if title:
            title_text = title.get_text()
            # é€šå¸¸æ ¼å¼æ˜¯ "skill-name by owner"
            name = title_text.split(" by ")[0].strip()
            return name

        return "unknown"

    def get_skill_detail_summary(self, detail: Dict) -> str:
        """
        è·å–æŠ€èƒ½è¯¦æƒ…æ‘˜è¦ï¼ˆç”¨äº AI åˆ†æï¼‰

        Args:
            detail: æŠ€èƒ½è¯¦æƒ…

        Returns:
            æ‘˜è¦æ–‡æœ¬
        """
        lines = [
            f"ã€æŠ€èƒ½åç§°ã€‘{detail.get('name')}",
            f"ã€æ‹¥æœ‰è€…ã€‘{detail.get('owner')}",
            f"ã€URLã€‘{detail.get('url')}",
        ]

        if detail.get("when_to_use"):
            lines.append(f"\nã€ç”¨é€”è¯´æ˜ã€‘")
            lines.append(detail.get("when_to_use"))

        if detail.get("rules"):
            lines.append(f"\nã€è§„åˆ™åˆ—è¡¨ã€‘({len(detail.get('rules'))} æ¡)")
            for rule in detail.get("rules")[:10]:  # æœ€å¤šæ˜¾ç¤º 10 æ¡
                lines.append(f"  - {rule.get('file')}: {rule.get('desc')}")

            if len(detail.get("rules")) > 10:
                lines.append(f"  ... è¿˜æœ‰ {len(detail.get('rules')) - 10} æ¡è§„åˆ™")

        return "\n".join(lines)


def fetch_details(skills: List[Dict]) -> List[Dict]:
    """ä¾¿æ·å‡½æ•°ï¼šè·å–æŠ€èƒ½è¯¦æƒ…"""
    fetcher = DetailFetcher()
    return fetcher.fetch_top20_details(skills)
